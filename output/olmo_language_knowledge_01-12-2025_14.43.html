<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OLMo's Proficiency in Foreign Languages: An Analytical Report</title>
    <style>
              body {
	              font-family: 'Arial', sans-serif;
	              margin: 20px;
	              line-height: 1.6;
	          }

        header {
            text-align: center;
            margin-bottom: 20px;
        }

        h1 {
            color: #333;
        }

        h2 {
            color: #444;
        }

        address {
            margin-top: 20px;
            font-style: normal;
        }

        section {
            margin-bottom: 30px;
        }

        summary {
            font-weight: bold;
            margin-bottom: 10px;
        }

        pre {
            white-space: pre-wrap;
        }
    </style>
</head>
<body>

    <header>
        <h1>OLMo's Proficiency in Foreign Languages: An Analytical Report</h1>
        <i>AUTONORA</i><br>
        <i>Allen Institute for AI, Seattle, WA</i></br>
	<i>01-12-2025 14:43:28</i><br>
	<tt>autonora@allenai.org</tt>
    </header>

<section>    
<h3>Abstract</h3>
<i>
<p>The goal of this research was to determine which foreign languages the language model OLMo knows best. To achieve this, we generated a dataset of 30 simple translation questions across multiple languages and collected OLMo's answers. These answers were then scored for accuracy using GPT-as-judge. The analysis focused on calculating average scores for each language and categorizing them based on linguistic families and difficulty levels to identify patterns in OLMo's performance.</p>

<p>Findings indicate that OLMo has the highest proficiency in Arabic, with a perfect average score, followed by Chinese. Languages were also categorized into groups such as Indo-European with Latin Script, Afro-Asiatic with Abjad Script, and others. This categorization revealed that OLMo performs exceptionally well in languages with logographic scripts and those belonging to the Afro-Asiatic family.</p>

<p>The significance of this work lies in its contribution to understanding the capabilities and limitations of language models like OLMo across different linguistic contexts. It provides insights into how such models can be further trained or utilized based on their strengths in specific language families or scripts.</p>
</i>
</section>
    

<section>
<h2>Introduction</h2>
<p>The burgeoning field of natural language processing has witnessed the advent of sophisticated language models capable of understanding and generating human-like text. These models, such as OLMo, have been trained on diverse datasets to perform a variety of linguistic tasks. However, the extent of their knowledge across different languages remains an area ripe for exploration. This research was motivated by the need to assess the proficiency of OLMo in translating simple English words into various foreign languages, which is crucial for applications requiring multilingual capabilities.</p>

<p>To this end, we embarked on a systematic investigation involving the creation of a dataset comprising 30 translation questions spanning multiple languages. OLMo was tasked with providing translations, which were subsequently evaluated for accuracy by GPT-as-judge. The scoring process allowed us to quantify OLMo's performance and identify which languages it translated most effectively. Additionally, we categorized languages based on linguistic families and difficulty levels to discern any emergent patterns in OLMo's performance across these categories.</p>

<p>Our main findings revealed that OLMo demonstrated the highest proficiency in Arabic and Chinese, suggesting an exceptional ability to handle translations into languages with logographic scripts and those within the Afro-Asiatic family. This introduction sets the stage for a detailed discussion on our methodology, analysis, and implications of these findings for future research and practical applications involving language models.</p>
</section>

<section>
  <h2>Approach</h2>
<p>The approach to determining OLMo's proficiency in foreign languages involved several methodical steps. Initially, a dataset was generated consisting of 30 translation questions, each requiring the translation of a simple English word into a different target language. The selection of languages aimed to cover a broad spectrum, including those with varying scripts, linguistic families, and levels of global influence.</p>

<p>Once the dataset was established, we employed OLMo to provide translations for each question. The model's responses were collected and added to the dataset for further evaluation. To assess the accuracy of OLMo's translations, we utilized GPT-as-judge - a method where another language model scores the correctness of answers on a scale from 0 (completely wrong) to 1 (perfectly correct). This scoring process also included justifications for each score assigned by GPT-as-judge, adding an additional layer of qualitative analysis.</p>

<p>After scoring, we analyzed the results by calculating the average score for each language. This quantitative measure served as an indicator of OLMo's translation proficiency per language. Furthermore, we ideated categories based on language difficulty levels and linguistic families such as Indo-European languages using Latin script or Afro-Asiatic languages using Abjad script. Each language in our dataset was mapped to one or more categories based on these criteria.</p>

<p>The final step involved aggregating scores within each category to determine how well OLMo performed across different types of languages. By comparing average scores among categories, we could identify patterns in performance related to specific linguistic features or complexities.</p>
</section>

<section>
<h2>Results</h2>
<p>The experimental results yielded insightful data on OLMo's translation capabilities across various languages. The scoring by GPT-as-judge revealed that OLMo achieved the highest average score in Arabic, with a perfect score of 1.00, indicating flawless translations for all questions in this language. Chinese also saw high proficiency with an average score of 1.00. These results suggest that OLMo is particularly adept at translating into languages with logographic scripts and those within the Afro-Asiatic family.</p>

<p>When examining performance across language families and scripts, the following average scores were observed:</p>
<ul>
<li>Indo-European languages using Latin script: 0.7875</li>
<li>Indo-European languages using Cyrillic script: 0.675</li>
<li>Japonic languages using Logographic script: 0.5</li>
</ul>

<p>This pattern indicates that OLMo's performance is not uniform across different linguistic categories and that certain language features may influence its translation accuracy.</p>

<p>In addition to individual language scores, the analysis of categories based on linguistic families and difficulty levels provided a broader perspective on OLMo's multilingual abilities. For instance, languages categorized under 'Afro-Asiatic' and 'Abjad Script' demonstrated exceptional results, while those under 'Indo-European' with 'Latin Script' showed good but not perfect performance.</p>

<p>The results from this study offer a quantitative basis for understanding how well current language models like OLMo can handle translations between English and a variety of foreign languages, highlighting areas of strength as well as potential for improvement.</p>
</section>

<section>
<h2>Analysis</h2>
<p>The analysis of the experimental results provides a nuanced understanding of OLMo's translation proficiency. The perfect scores in Arabic and Chinese suggest that OLMo is highly capable of handling translations into languages with non-Latin scripts, which may be attributed to specific training or inherent model capabilities that handle the complexity of logographic systems well. For example, the translation of 'peace' into Arabic ('') and 'happiness' into Chinese ('') were both accurate, reflecting a strong grasp of semantic meaning across diverse linguistic structures.</p>

<p>However, the performance was not uniform across all language families and scripts. While Indo-European languages using Latin script received high scores overall, they were not perfect. This could indicate that despite sharing a common script, there are nuances within these languages that pose challenges for OLMo. An illustrative example is the translation of 'water' into German ('Wasser'), which is straightforward, versus more complex words that may involve gender or case variations.</p>

<p>Languages using Cyrillic script showed lower average scores compared to those using Latin script. This might be due to less exposure or training data for these languages during OLMo's development. For instance, translating 'book' into Russian ('') might have been successful, but other words could present difficulties related to Cyrillic orthography or grammar.</p>

<p>The Japonic language category had an average score indicating moderate performance. This result suggests that while OLMo can handle some translations into Japanese effectively, there may be limitations when dealing with the intricacies of its logographic script combined with phonetic characters (kanji and kana). A correct translation like 'mountain' into Japanese ('') shows competence, but this does not necessarily extend to all words in this category.</p>

<p>Overall, the analysis underscores that while OLMo exhibits strong translation abilities in certain contexts, there are variances based on language family and writing system that impact its performance. These findings highlight areas where additional training data or focused learning could enhance OLMo's multilingual capabilities.</p>
</section>

<section>
<h2>Conclusion</h2>
<p>The research conducted to evaluate OLMo's proficiency in foreign languages has led to several key conclusions. Firstly, OLMo demonstrated exceptional translation accuracy for Arabic and Chinese, achieving perfect average scores. This indicates a robust ability to handle translations into languages with logographic scripts and those within the Afro-Asiatic family. Secondly, while OLMo performed well across Indo-European languages using Latin script, it did not achieve perfection, suggesting room for improvement in handling the subtleties of these languages.</p>

<p>Thirdly, the performance across Cyrillic script languages was lower than that of Latin script languages, which may reflect disparities in training data or model familiarity with these scripts. Finally, moderate performance in Japonic languages suggests that OLMo can manage some aspects of these translations but may struggle with the complexity of combining logographic and phonetic systems.</p>

<p>In conclusion, our findings provide valuable insights into the capabilities and limitations of language models like OLMo across different linguistic contexts. The results underscore the importance of considering language families and scripts when evaluating and utilizing language models for multilingual applications. They also point towards potential areas where targeted improvements could enhance model performance for specific language groups.</p>
</section>

<h3>Notes</h3>
69 GPT calls, 30 OLMo calls, 0 Together calls.
Runtime: 7 minutes.




